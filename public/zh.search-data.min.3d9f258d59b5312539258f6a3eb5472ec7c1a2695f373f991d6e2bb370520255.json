[{"id":0,"href":"/docs/%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80/MATH1-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/","title":"Math1 线性代数","section":"数学基础","content":"\u003c!doctype html\u003e\rMATH1-线性代数\r线性代数向量和向量空间向量标量(scalar)是一个实数，一般用斜体小写字母a,b,c来表示。向量(vector)是由一组实数组成的有序数组，一个n维向量a由n个有序实数组成，表示为a=[a1,a2,⋯,an]，其中ai称为向量a的第i个分量(第i维)。\n# numpy库常用于实现线性代数中向量和矩阵的基本操作import numpy as np# numpy中向量的定义v_1 = np.array([1, 2, 3, 4, 5])v_2 = np.array([5.6, 4.6, 3.6, 2.6, 1.6])向量空间向量空间(vector space)也称线性空间(linear space)，是指由向量组成的集合，并满足以下两个条件：\n(1) 向量加法封闭性：向量空间V中的任意两个向量a和b，它们的和a+b也属于向量空间V；\n# numpy实现向量加法v_a = np.add(v_1, v_2)print(v_a, v_a.shape) \u0026nbsp;# 结果为向量(2) 标量乘法封闭性：向量空间V中的任一向量a和任一标量c，它们的乘积ca也属于向量空间V。\n# numpy实现向量与标量相乘k = 2.0v_k2 = k * v_1print(v_k2, v_k2.shape) \u0026nbsp;# 结果为向量一个常用的线性空间是欧式空间(Euclidean space)，常表示为Rn，其中n为空间维度(dimension)。欧式空间中的向量加法和标量乘法定义为：\n(1)[a1,a2,⋯,an]+[b1,b2,⋯,bn]=[a1+b1,a2+b2,⋯,an+bn]c⋅[a1,a2,⋯,an]=[ca1,ca2,⋯,can]线性子空间：向量空间V的线性子空间U是V的一个子集，并且满足向量空间的条件。\n设v1,v2,⋯,vn为向量空间V中的向量，则其线性组合a1v1+a2v2+⋯+anvn构成V的子空间，并将其称为向量v1,v2,⋯,vn张成(span)的子空间，或v1,v2,⋯,vn的张成，记作span(v1,v2,⋯,vn)。\n线性无关：线性空间V中的一组向量{v1,v2,⋯,vn}，如果对任意的一组标量λ1,λ2,⋯,λn，若∑iλivi=0，则必然λ1=λ2=⋯=λn=0，那么{v1,v2,⋯,vn}是线性无关的，也称为线性独立的。\n基向量：线性空间V的基(base)B={e1,e2,⋯,en}是V的有限子集，其元素之间线性无关。向量空间V中的所有向量都可以按唯一的方式表达为B中向量的线性组合。即对任意v∈V，存在一组标量(λ1,λ2,⋯,λn)，使得：\n(2)v=λ1e1+λ2e2+⋯+λnenB中的向量称为基向量(base vector)。(λ1,λ2,⋯,λn)称为向量v关于基B的坐标(coordinate)。向量空间中基的个数即向量空间的维数。\n内积(inner product)：一个n维线性空间中的两个向量a和b，其内积(也称点积)为：\n(3)⟨a,b⟩=aTb=∑i=1naibi# numpy实现向量内积v_i = np.inner(v_1, v_2)print(v_i, v_i.shape) \u0026nbsp;# 结果为标量向量内积实际上是矩阵乘法的一种特例。向量a和b的外积(outer product)定义为：\n(4)abT=[a1a2⋮am][b1b2⋯bn]=[a1b1a1b2⋯a1bna2b1a2b2⋯a2bn⋮⋮⋱⋮amb1amb2⋯ambn]# numpy实现向量外积v_o = np.outer(v_1, v_2)print(v_o, v_o.shape) \u0026nbsp;# 结果为矩阵正交(orthogonal)：如果两个向量的内积为0，则它们正交。如果一个向量v与子空间U中的每个向量都正交，那么向量v与子空间U正交。\n范数范数(norm)是一个表示向量“长度”的函数，为向量空间内所有向量赋予非零的正长度或大小。对n维向量v，一个常见的范数函数为ℓp范数：\n(5)ℓp(v)≡∥v∥p=(∑i=1n|vi|p)1/p# numpy实现L2范数v_1_2 = v_1 * v_1 \u0026nbsp;# '*'运算符代表向量对应位置相乘(即内积)sum_l2 = np.sum(v_1_2)l2 = np.sqrt(sum_l2)print(l2)其中p⩾0为一个标量的参数。当p=1时，ℓ1范数为向量的各元素绝对值之和，称为曼哈顿距离；p=2时，ℓ2范数为向量的各元素的平方和再开平方，称为欧氏距离。ℓ∞范数为向量的各个元素的最大绝对值。下图给出了常见范数的示例，其中红线表示不同范数ℓp=1的点：\n矩阵线性映射线性映射(linear mapping)是指从线性空间V到线性空间W的一个映射函数f:V→W，并满足：对于V中任何两个向量u和v以及任何标量c，有：\n(6)f(u+v)=f(u)+f(v)f(cv)=cf(v)即该函数对加法和数量乘法封闭。两个有限维欧式空间的映射函数f:Rn→Rm可以表示为：\n(7)y=Ax≜[a11x1+a12x2+⋯+a1nxna21x1+a22x2+⋯+a2nxn⋮am1x1+am2x2+⋯+amnxn]其中A定义为m×n的矩阵(matrix)，是一个由m行n列元素排列成的矩形阵列。一个矩阵A从左上角数起的第i行第j列上的元素称为第i,j项，通常记为[Aij]或aij。矩阵A定义了一个从Rn到Rm的线性映射，向量x和y分别为两个空间中的列向量，即大小为n×1或m×1的矩阵：\n(8)x=[x1x2⋮xn],y=[y1y2⋮ym]在没有特殊说明的情况下，向量默认为列向量，且行向量表示为[x1,x2,⋯,xn]，列向量表示为[x1;x2;⋯;xn]或行向量的转置[x1,x2,…,xn]T。\n# numpy中矩阵的定义A = [[1, 2, 3, 4], \u0026nbsp; \u0026nbsp; [5, 6, 7, 8], \u0026nbsp; \u0026nbsp; [9, 10, 11, 12]] \u0026nbsp;# (3,4)A = np.array(A)print(A.shape)\rB = [[1, 2, 3, 4, 5], \u0026nbsp; \u0026nbsp; [6, 7, 8, 9, 10], \u0026nbsp; \u0026nbsp; [11, 12, 13, 14, 15], \u0026nbsp; \u0026nbsp; [16, 17, 18, 19, 20]] \u0026nbsp;# (4,5)B = np.array(B)print(B.shape)\r# 满秩矩阵X = [[2, 6, 9], \u0026nbsp; \u0026nbsp; [1, 9, 3], \u0026nbsp; \u0026nbsp; [7, 2, 4]]矩阵操作(1) 加法：[A+B]ij=aij+bij，必须保证运算的两个矩阵的大小相同。\n# numpy实现矩阵加法print(np.add(A, A))(2) 乘积：[AB]ij=∑k=1maikbkj，必须保证第一个矩阵的列数和第二个矩阵的行数相等。矩阵的乘积表示一个复合线性映射，即先完成线性映射B，再完成线性映射A。如果A是k×m阶矩阵，B是m×n阶矩阵，则其乘积AB是一个k×n阶矩阵。矩阵乘法满足结合律和分配率：\n结合律：(AB)C=A(BC)，\n分配率：(A+B)C=AC+BC,C(A+B)=CA+CB。\nxxxxxxxxxx# numpy实现矩阵乘法C = np.dot(A, B)print(C, C.shape) \u0026nbsp;# (3,4) * (4,5) = (3,5)(3) Hadamard积：[A⊙B]ij=aijbij，即A和B中对应的元素相乘，必须保证运算的两个矩阵的大小相同。\nxxxxxxxxxx# numpy实现Hadamard积print(np.multiply(A, A))(4) 转置(transposition)：[AT]ij=[A]ji。显然，(A+B)T=AT+BT，(AB)T=BTAT。\nxxxxxxxxxx# numpy实现矩阵转置A_T = np.transpose(A)print(A_T, A_T.shape)(5) 迹(trace)：对于n阶方阵A，它的迹为主对角线上的元素之和，记作tr(A)=∑i=1nAii。迹有如下性质：\n(9)tr(AT)=tr(A)tr(A+B)=tr(A)+tr(B)tr(AB)=tr(BA)tr(ABC)=tr(BCA)=tr(CAB)xxxxxxxxxx# numpy计算方阵的迹print(np.trace(A)) \u0026nbsp;# 结果为标量(6) 行列式(determinant)：n阶方阵A的行列式定义为det(A)=∑σ∈Snpar(σ)A1σ1A2σ2⋯Anσn，其中Sn为所有n阶排列(permutation)的集合，par(σ)的值为-1或+1取决于σ为及排列或偶排列，即其中出现降序的次数为奇数或偶数，例如(1,3,2)中降序次数为1，(3,1,2)中降序次数为2。单位阵的行列式为det(I)=1。\nxxxxxxxxxx# numpy计算方阵的行列式print(np.linalg.det(X))print(np.linalg.det(A)) \u0026nbsp;# 报错，计算行列式的矩阵必须为方阵n阶方阵A的行列式有如下性质：\n如果行列式中有一行为零，或有两行相同，或有两行成比例，那么行列式为零；\n对换行列式中两行的位置，行列式反号；\n把一行的倍数加到另一行，行列式不变；\n行列式的其他运算性质如下：\n(10)det(cA)=cndet(A)det(AT)=det(A)det(AB)=det(A)det(B)det(A−1)=det(A)−1det(An)=det(A)n低维矩阵的行列式计算举例如下：\n(11)|[a11]|=a11|[a11a12a21a22]|=a11a22−a12a21|[a11a12a13a21a22a23a31a32a33]|=a11a22a33+a12a23a31+a13a21a32−a11a23a32−a12a21a33−a13a22a31(7) 秩(rank)：一个矩阵A的列秩是A的线性无关的列向量的数量，行秩是A的线性无关的行向量数量。一个矩阵的列秩和行秩总是相等的，简称为秩。一个m×n阶矩阵A的秩最大为min(m,n)。若rank(A)=min(m,n)，则称矩阵A是满秩的。如果一个矩阵不满秩，说明其包含线性相关的列向量或行向量，其行列式为0。\nxxxxxxxxxx# numpy计算矩阵的秩print(np.linalg.matrix_rank(X))print(np.linalg.matrix_rank(A))两个矩阵的乘积AB的秩rank(AB)⩽min(rank(A),rank(B))。\n矩阵A的秩=矩阵A的列秩=矩阵A的行秩，矩阵的初等变换皆不改变矩阵的秩、行秩和列秩。\n(8) 范数(norm)：与向量的范数相似，矩阵常用的ℓp范数定义为：\n(12)∥A∥p=(∑i=1m∑j=1n|aij|p)1/p(9) 矩阵的逆(inverse matrix)：对于n×n的方阵A，如果存在另一个方块矩阵B使得AB=BA=In，其中In为单位矩阵，则称A是可逆的。矩阵B称为矩阵A的逆矩阵，记为A−1。矩阵的逆满足如下性质：\n(13)(A−1)−1=A(AB)−1=B−1A−1(A−1)T=(AT)−1一个方阵的行列式等于0当且仅当该方阵不可逆。不可逆矩阵也称奇异矩阵，可逆矩阵也称非奇异矩阵。对非方阵或奇异矩阵，可以计算其伪逆。\n求A−1的算法：构造增广矩阵[A,I]，进行行化简，若A行等价于I，则[A,I]行等价于[I,A−1]，否则A没有逆。\nxxxxxxxxxx# numpy计算方阵的逆print(np.linalg.inv(X)) \u0026nbsp;# print(X.I)也可以print(np.linalg.inv(A)) \u0026nbsp;# 报错，计算逆的矩阵必须为方阵\r# numpy计算矩阵的伪逆M = np.zeros((4, 4)) \u0026nbsp;# 定义一个奇异阵MM[0, -1] = 1M[-1, 0] = -1M = np.matrix(M)print(M)# print(M.I) # 将报错，矩阵M为奇异矩阵，不可逆print(np.linalg.pinv(M)) \u0026nbsp;# 求矩阵M的伪逆(广义逆矩阵)(10) 初等行变换：对于矩阵A，把A的某一行所有元素乘以一非零元素，或把A的两行互换，或把A的某一行换乘它本身与另一行的倍数的和，这三种操作称为矩阵A的初等行变换。若矩阵A经过有限次初等行变换后可以转化为矩阵B，则称矩阵A和矩阵B是行等价的。\n定理：n×n矩阵A是可逆的，当且仅当A行等价于单位矩阵In。这时，将A化简为In的一系列初等行变换也可以将In转换为A−1。\n证明：A∼E1A∼E2(E1A)∼⋯∼Ep(Ep−1⋯E1A)=In，因此EpEp−1⋯E1In=A−1。\n(11) Numpy中矩阵运算的广播机制：\nxxxxxxxxxx# 通常情况下，numpy两个数组的相加、相减以及相乘都是对应元素之间的操作x = np.array([[2, 2, 3], [1, 2, 3]])y = np.array([[1, 1, 3], [2, 2, 4]])print(x * y)\r# 当两个张量维度不同，numpy可以自动使用广播机制使得运算得以完成。例如：arr = np.random.randn(4, 3) \u0026nbsp;# (4,3)arr_mean = arr.mean(axis=0) \u0026nbsp;# shape(3,)demeaned = arr - arr_mean \u0026nbsp;# (4,3) - (3,)print(demeaned)\r# 广播的原则：如果两个数组的后缘维度（trailing dimension，即从末尾开始算起的维度）的轴长度相符，或其中的一方的长度为1，# 则认为它们是广播兼容的。广播会在缺失和(或)长度为1的维度上进行。\r# 广播主要发生在两种情况，一种是两个数组的维数不相等，但是它们的后缘维度的轴长相符，另外一种是有一方的长度为1。\r# 1. 数组维度不同，后缘维度的轴长相符arr1 = np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3]]) \u0026nbsp;# (4,3)arr2 = np.array([1, 2, 3]) \u0026nbsp;# (3,)arr_sum = arr1 + arr2print(arr_sum)# 在上例中，(4,3) + (3,) = (4,3)。类似的例子还有：(3,4,2) + (4,2) = (3,4,2)\r# 2. 有一方的长度为1arr1 = np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3]]) \u0026nbsp;# (4,3)arr2 = np.array([[1], [2], [3], [4]]) \u0026nbsp;# (4,1)arr_sum = arr1 + arr2print(arr_sum)# 在上例中，(4,3) + (4,1) = (4,3)类似的例子还有：(4,6) + (1,6) = (4,6)；(3,5,6) + (1,5,6) = (3,5,6)；# (3,5,6) + (3,1,6) = (3,5,6)；(3,5,6) + (3,5,1) = (3,5,6)等矩阵类型(1) 对称矩阵(symmetric matrix)：转置等于其自己的矩阵，即满足A=AT。若A=−AT，则称矩阵A为反对称矩阵(anti-symmetric matrix)。易证，对于矩阵A∈Rm×n，A+AT为对称矩阵，A−AT为反对称矩阵。从以上结论又可以得出，任意方阵(square matrix)A∈Rn×n都可以由一个对称矩阵和一个反对称矩阵表示：\n(14)A=12(A+AT)+12(A−AT)对称矩阵在实际应用中有很多良好的性质，通常将n维对称矩阵的集合记为Sn。\n(2) 对角矩阵(diagonal matrix)：除了主对角线外的元素皆为0的矩阵，对角线上的元素可以为0或其他值。一个n×n的对角矩阵A满足：[A]ij=0\u0026nbsp;\u0026nbsp;\u0026nbsp;if\u0026nbsp;i≠j,∀i,j∈{1,⋯,n}。\n对角矩阵也可以记作diag(a)，其中a为一个n维向量，并满足[A]ii=ai。一个n×n阶对角矩阵A=diag(A)和n维向量b的乘积为一个n维向量：Ab=diag(a)b=a⊙b，其中⊙表示点乘。\n(3) 单位矩阵(identity matrix)：一种特殊的对角矩阵，其主对角线元素为1，其余元素为0。n阶单位矩阵In是一个n×n的方块矩阵，可以记为In=diag(1,1,⋯,1)。\n一个m×n的矩阵A和单位矩阵的乘积(左乘积和右乘积)等于其本身：AIn=ImA=A。\n(4) 正定矩阵(positive-definite matrix)：对于一个n×n阶的对称矩阵A，如果对于所有的非零向量x∈Rn，都满足xTAx\u0026gt;0，则A为正定矩阵。如果xTAx\u0026nbsp;⩾0，则A为半正定矩阵(positive-semidefinite matrix)。\n(5) 正交矩阵(orthogonal matrix)：对于方阵U∈Rn×n，如果UUT=I或UTU=I，则称U为正交矩阵。\n(6) Gram矩阵：向量空间中一组向量v1,v2,⋯,vn的Gram矩阵G是内积的对称矩阵，其元素Gij=viTvj。\n相似矩阵与对角化设A和B是n×n矩阵，如果存在可逆矩阵P，使得P−1AP=B，则称A相似于B，B也相似于A。将A变成P−1AP的变换称为相似变换。\n若A和B是相似的，那么它们有相同的特征多项式，有相同的特征值；但是有相同的特征值的矩阵并不一定相似。\n证明：P−1AP=B，则|B−λI|=|P−1AP−λI|=|P−1(A−λI)P|=|A−λI|。\n若方阵A相似于对角矩阵，则称A可对角化。n×n矩阵A可对角化的充分必要条件是A有n个线性无关的特征向量。\n二次型与正定矩阵给定一个方阵A∈Rn×n和一个列向量x∈Rn，标量xTAx被称为一个二次型(quadratic form)。具体地：\n(15)xTAx=∑i=1nxi(Ax)i=∑i=1nxi(∑j=1nAijxj)=∑i=1n∑j=1nAijxixj值得注意的是：\n(16)xTAx=(xTAx)T=xTATx=xT(12A+12AT)x其中，第一个等式依据标量的转置等于其自身的事实，而第二个等式源自以下事实：我们对两个本身相等的量求平均。 据此可以得出结论，只有A的对称部分对二次形有帮助。 由于这个原因，我们经常隐式地假设以二次形式出现的矩阵是对称的。\n正定矩阵、半正定矩阵、负定矩阵、半负定矩阵以及不定矩阵的定义如下：\n一个关于正定矩阵或负定矩阵的重要性质是，其总是满秩的，因此一定是可逆矩阵。证明如下：\n假定一个矩阵A∈Rn×n是不满秩的，并假设A的第j列可以由其他n−1列进行线性表示，即aj=∑i≠jxiai。其中，x1,⋯,xj−1,xj+1,⋯,xn∈R是一系列标量。当设置xj=−1时，有：\n(17)Ax=∑i=1nxiai=0对于某些非零向量x，上式会使得xTAx=0，与正定或负定矩阵的定义矛盾。证毕。\n特征值与特征向量对矩阵A，如果存在一个标量λ和一个非零向量x满足Ax=λx，则λ和x分别称为矩阵A的特征值(eigenvalue)和特征向量(eigenvector)。对于任意特征向量x和标量c，A(cx)=λ(cx)，因此cx也是特征向量。因此，在通常情况下，仅讨论模为1的特征向量。矩阵A可以认为是一个变换，这个变换的特殊之处是，当它作用在特征向量x上的时候，x只产生了缩放变换，并没有产生旋转变换。\n上式还可以变形为：\n(18)(λI−A)x=0,x≠0这是含有n个未知数的n个方程的齐次线性方程组，它有非零解的充分必要条件是系数行列式|(λI−A)|=0。将该等式变形，会得到关于λ的多项式。将多项式进行求解后，会得到最多n个特征值λ。将特征值分别代入原式，便可以得到最多n个特征向量。这种方式仅仅作为手工求解特征值和特征向量的方法，并不被实际应用。\n以下是关于特征值的一些性质。其中A∈Rn×n，λ1⋯,λn是n个特征值，对应n个特征向量。x1,⋯,xn。\n可以将所有特征向量和特征值合并成矩阵形式，并写为：AX=XΛ。其中，\n(19)X∈Rn×n=[|||x1x2⋯xn|||],Λ=diag(λ1,…,λn)若矩阵A的特征向量线性无关，则矩阵X是可逆矩阵，则A=XΛX−1。该过程称为对角化(diagonalization)。\nxxxxxxxxxx# numpy实现矩阵特征值分解X = [[1, 2, 3], \u0026nbsp; \u0026nbsp; [4, 5, 6], \u0026nbsp; \u0026nbsp; [7, 8, 9]]\reigenvalues, eigenvectors = np.linalg.eig(X)\rprint(\"eigenvalue: \\n\", eigenvalues)print(\"eigenvector: \\n\", eigenvectors)奇异值分解奇异值分解的定义与性质任意一个m×n阶矩阵，都可以表示为三个矩阵的乘积，分别是m阶正交矩阵、由降序排列的非负的对角线元素组成的m×n阶对角矩阵和n阶正交矩阵，称为该矩阵的奇异值分解(singular value decomposition, SVD)。一个矩阵的奇异值分解一定存在，但不唯一。\n奇异值分解可以看作是矩阵数据压缩的一种方法，即用因子分解的方式近似地表示原始矩阵，这种近似是在平方损失意义下的最优近似。\n定义与定理矩阵的奇异值分解是指将一个非零的m×n阶实矩阵A∈Rm×n表示为三个实矩阵乘积形式的运算A=UΣVT。其中，U是m阶正交矩阵，V是n阶正交矩阵，Σ是降序排列的非负对角线元素组成的m×n阶对角矩阵，满足UUT=I，VVT=I，Σ=diag(σ1,σ2,⋯,σp)(σi非负且降序排列)，p=min(m,n)。UΣVT称为矩阵A的奇异值分解，σi称为矩阵A的奇异值，U的列向量称为左奇异向量，V的列向量称为右奇异向量。\n注意：奇异值分解不要求A是方阵，事实上，奇异值分解可以看作方阵对角化的推广。\n奇异值分解基本定理：若A为实矩阵，则A的奇异值分解一定存在。\n紧奇异值分解与截断奇异值分解A=UΣVT又称为矩阵的完全奇异值分解，实际常用的是紧凑形式和截断形式。\n(1) 紧奇异值分解：设m×n阶实矩阵A的秩rank(A)=r,r⩽min(m,n)，则称UrΣrVrT为矩阵A的紧奇异值分解。其中Ur是m×r矩阵，Vr是n×r矩阵，Σr是r阶对角矩阵。\n(2) 截断奇异值分解：在矩阵的奇异值分解中，只取最大的k个奇异值(k\u0026lt;r，r为矩阵的秩)对应的部分，就得到矩阵的截断奇异值分解。实际应用中提到矩阵的奇异值分解时，通常指截断奇异值分解。此时Σk是k阶对角矩阵。\n几何解释三个矩阵(对应一个线性变换)可以理解为三个线性变换的步骤：一个坐标系的旋转变换、一个坐标轴的缩放变换和一个坐标系的旋转或反射变换。\n奇异值分解的计算奇异值分解的计算过程如下：\n(1) 首先求ATA的特征值，记作λ1,⋯,λn，共有n个，然后求解出对应的特征向量x1,⋯,xn。\n(2) 求n阶正交矩阵V：将特征向量单位化，得到单位特征向量v1,⋯,vn构成n阶正交矩阵V=[v1\u0026nbsp;v2\u0026nbsp;⋯\u0026nbsp;vn]。\n(3) 求m×n阶对角矩阵Σ：计算A的奇异值σi=λi,i=1,2,⋯,n(奇异值就是ATA的特征向量的平方根)，构造m×n阶对角矩阵Σ，主对角线元素是奇异值，其余元素为0。\n(4) 求m阶正交矩阵U：对A的前r个正奇异值，令uj=(1/σj)Avj,j=1,2,⋯,r，得到U1=[u1\u0026nbsp;u2⋯\u0026nbsp;ur]。求AT的零空间(即ATx=0的解空间)一组标准正交基{ur+1,ur+2,⋯,um}，令U2=[ur+1,ur+2,⋯,um]并令U=[U1\u0026nbsp;U2]，得到正交矩阵U。\n从上述算法可以看出，奇异值分解的关键在于的ATA特征值的计算。实际应用中的奇异值分解算法是通过求解ATA的特征值，但不直接计算ATA。按照这个思路，产生了许多矩阵奇异值分解的有效算法。\nxxxxxxxxxx# numpy实现矩阵的奇异值分解U, Sigma, V = np.linalg.svd(X, )\rprint(\"U: \\n\", U)print(\"Sigma: \\n\", Sigma)print(\"V: \\n\", V)\r# 通过截断奇异值分解重建矩阵XSigma[1:] = 0X_rebuild = np.mat(U) * np.mat(np.diag(Sigma)) * np.mat(V)print(X_rebuild) \u0026nbsp;# 矩阵X的近似\r# 奇异值分解在图像处理中的应用import matplotlib.pyplot as plt\rlenna = plt.imread('lenna.jpg') \u0026nbsp;# 需要预先将图片lenna.jpg放入当前目录中print(lenna.shape) \u0026nbsp;# (2318,1084,3)\rlenna = lenna[:1000, :1000, 2] \u0026nbsp;# 将图像大小调整为(1000,1000)，且仅选取一个通道print(lenna.shape) \u0026nbsp;# (1000,1000)\rU, Sigma, V = np.linalg.svd(lenna)print(U.shape, Sigma.shape, V.shape)\r# 通过截断奇异值分解重建莱娜图，观察k取不同值时的重建结果k = [1000, 500, 300, 200, 100, 50]for i in range(len(k)): \u0026nbsp; \u0026nbsp;Sigma_k = np.copy(Sigma) \u0026nbsp; \u0026nbsp;Sigma_k[k[i]:] = 0 \u0026nbsp; \u0026nbsp;lenna_rebuild = np.mat(U) * np.mat(np.diag(Sigma_k)) * np.mat(V)\r\u0026nbsp; \u0026nbsp;plt.subplot(2, 3, i + 1) \u0026nbsp; \u0026nbsp;plt.title('truncated k={}'.format(k[i])) \u0026nbsp; \u0026nbsp;plt.imshow(lenna_rebuild)\rplt.show()LU分解LU分解(LU decomposition)是矩阵分解的一种，可以将一个矩阵分解为一个单位下三角矩阵和一个上三角矩阵的乘积(有时是它们和一个置换矩阵的乘积)。LU分解主要应用在数值分析中，用来解线性方程、求反矩阵或计算行列式。本质上，LU分解是高斯消元的一种表达方式。首先，对矩阵A通过初等行变换将其变为一个上三角矩阵；然后，将原始矩阵A变为上三角矩阵的过程，对应的变换矩阵为一个下三角矩阵。这中间的过程，就是Doolittle algorithm(杜尔里特算法)。\n在求解线性方程组Ax=b时，求解时间为23n3。将矩阵A进行LU分解(复杂度23n3)后，可以将线性方程组转换为Ly=b和Ux=y，二者的计算复杂度均为n2，运算速度显著提升。\nxxxxxxxxxx# numpy实现矩阵LU分解import numpy as np\rdef lu_decomposition(A): \u0026nbsp; \u0026nbsp;n = len(A[0]) \u0026nbsp; \u0026nbsp;L = np.zeros([n, n]) \u0026nbsp; \u0026nbsp;U = np.zeros([n, n]) \u0026nbsp; \u0026nbsp;for i in range(n): \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;L[i][i] = 1 \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;if i == 0: \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;U[0][0] = A[0][0] \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;for j in range(1, n): \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;U[0][j] = A[0][j] \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;L[j][0] = A[j][0] / U[0][0] \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;else: \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;for j in range(i, n): \u0026nbsp;# U \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;temp = 0 \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;for k in range(0, i): \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;temp = temp + L[i][k] * U[k][j] \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;U[i][j] = A[i][j] - temp \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;for j in range(i + 1, n): \u0026nbsp;# L \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;temp = 0 \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;for k in range(0, i): \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;temp = temp + L[j][k] * U[k][i] \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp;L[j][i] = (A[j][i] - temp) / U[i][i] \u0026nbsp; \u0026nbsp;return L, U\rA = np.array([[4., -1., -1., 0., 0., 0., 0., 0.], \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; [-1., 4., -1., -1., 0., 0., 0., 0.], \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; [-1., -1., 4., -1., -1., 0., 0., 0.], \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; [0., -1., -1., 4., -1., -1., 0., 0.], \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; [0., 0., -1., -1., 4., -1., -1., 0.], \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; [0., 0., 0., -1., -1., 4., -1., -1.], \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; [0., 0., 0., 0., -1., -1., 4., -1.], \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; [0., 0., 0., 0., 0., -1., -1., 4.]])L, U = lu_decomposition(A)print(L.tolist())print(U.tolist())参考资料李航. 统计学习方法. 北京: 清华大学出版社, 2019.\n周志华. 机器学习. 北京: 清华大学出版社, 2016.\n邱锡鹏. 神经网络与深度学习. 北京: 机械工业出版社, 2020.\nStanford University机器学习课程：http://cs229.stanford.edu/\nNumpy官方文档：https://numpy.org/doc/stable/index.html#\n\u0026nbsp;\n"}]